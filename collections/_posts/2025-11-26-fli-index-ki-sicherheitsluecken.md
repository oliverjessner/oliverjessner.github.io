---
layout: post
title: 'Schlechte Noten für KI-Giganten: FLI-Index zeigt Sicherheitslücken, EU AI Act erhöht den Druck'
date: 2025-11-26 12:10:10 +0100
authors: ['oliver_jessner']
meta_og_type: 'article'
categories:
    - KI
description: 'Neuer Future-of-Life-Institute-Index 2025 offenbart gravierende Sicherheitsdefizite bei Anthropic, OpenAI & Co. – die EU will mit dem AI Act gegensteuern.'
thumbnail: '/assets/images/gen/blog/fli-index-ki-sicherheitsluecken/header_thumbnail.webp'
image: '/assets/images/gen/blog/fli-index-ki-sicherheitsluecken/header.webp'
---

# FLI-Index 2025: KI-Firmen fallen im Sicherheitsrating durch

Neue Zahlen des Future-of-Life-Institute (FLI) machen KI-Risikoforscher nervös:  
Der im Juli 2025 veröffentlichte Sicherheitsindex bewertet sieben Top-Firmen – mit ernüchternden Ergebnissen.

-   **Anthropic** erreicht als einziger Anbieter ein **„C+“**
-   **OpenAI**, **Google DeepMind** und **x.ai** landen bei **„C“ bis „D“**
-   **Meta** verpasst knapp die Mindestanforderungen
-   **DeepSeek** fällt mit **„F“** komplett durch

Die Experten warnen deshalb vor einem blinden Wettlauf und fordern eine starke, verbindliche Aufsicht.

---

## Index legt Schwächen offen

Der neue Sicherheitsindex des Future-of-Life-Institute  
([Quelle](https://futureoflife.org/ai-safety-index-summer-2025/))  
liefert ein Zeugnis für die gängigen generativen KI-Systeme. Ein Panel aus sechs Sicherheits- und Governance-Fachleuten bewertete:

-   Anthropic
-   OpenAI
-   Google DeepMind
-   x.ai
-   Meta
-   Zhipu AI
-   DeepSeek

Anhand von **34 messbaren Indikatoren**.

Die Datenbasis bestand aus:

-   öffentlichen Dokumenten
-   vertraulichen Fragebögen
-   eigenen Recherchen des Panels

Die Benotung folgt dem US-Schulsystem.

**Ergebnisübersicht:**

| Unternehmen     | Punkte | Note |
| --------------- | ------ | ---- |
| Anthropic       | 2,64   | C+   |
| OpenAI          | 2,10   | C    |
| Google DeepMind | 1,76   | C–   |
| x.ai            | 1,23   | D    |
| Meta            | 1,06   | D    |
| Zhipu AI        | 0,62   | F    |
| DeepSeek        | 0,37   | F    |

Schon dieser Überblick zeigt das Missverhältnis zwischen **technischer Ambition** und **nachweisbarer Sicherheitsreife**.

---

## Leichte Fortschritte, große Lücken

Im Vergleich zum Vorjahr gibt es leichte Verbesserungen:

-   **Anthropic:** von „C“ → „C+“
-   **OpenAI:** von „D+“ → „C“
-   **Google DeepMind:** von „D+“ → „C–“
-   **x.ai:** von „D–“ → „D“
-   **Meta:** von „F“ → „D“
-   **Zhipu AI:** von „D“ → „F“ (Rückschritt)

DeepSeek war im Vorjahr noch nicht bewertet.

Doch echte Risikominderung zeigt erst der Blick auf die **34 Prüfsteine**.

---

## Methodik des Index

FLI ordnet die 34 Indikatoren sechs Bereichen zu:

1. **Risikoanalysen**
2. **Aktuelle Schäden**
3. **Sicherheitsframeworks**
4. **Existenzsicherheit**
5. **Governance & Verantwortung**
6. **Informationsteilung**

Alle Kategorien fließen **gleich gewichtet** in die Endnote ein.

Neu im Fokus:

-   gefährliche Fähigkeiten (Bio-, Cyberrisiken)
-   veröffentlichte Whistleblowing-Regeln
-   dokumentierte Abschaltkriterien

Bewertet wurde im **Delphi-Verfahren**. Externe Prüfer glichen Selbstberichte mit offenen Quellen ab.  
Teilweise erhielten sie Zugriff auf unveröffentlichte Protokolle, z. B. aus Tests zur Instruktionstreue.

Der Bericht selbst räumt ein:

> Fehlende Transparenz schmälert die Aussagekraft.  
> Mehrere Teilnoten haben niedrige Konfidenzgrade.

Gerade diese Unsicherheiten machen deutlich, wie wichtig unabhängige Aufsicht wird.

---

## Anthropic vorn, China hinten

Bemerkenswert ist, dass **Anthropic trotz kleinerer Ressourcen** vorn liegt:

-   kein Training auf Nutzerdaten
-   intensive Alignment-Forschung
-   Unternehmensform als **Public-Benefit-Corporation**

**OpenAI** erhält Punkte für:

-   als Einziger ein **veröffentlichtes Whistleblowing-Reglement**
-   detaillierte Modellkarten

Verliert aber Profil nach Auflösung des Superalignment-Teams.

**Google DeepMind:**

-   stark bei internem Red-Teaming
-   schwach bei externer Transparenz

**x.ai** und **Meta**:

-   bleiben in den meisten Kategorien unterdurchschnittlich
-   kaum belastbare Zahlen zu Bio- oder Cyberrisiken

**Zhipu AI** und **DeepSeek**:

-   lassen zwei Drittel der Fragen unbeantwortet
-   erhalten deshalb ein **„F“**

**Kernaussage:**  
Ohne Transparenz bleiben selbst gute interne Verfahren unsichtbar – und nicht überprüfbar.

---

## Existenzrisiken systematisch unterschätzt

Am schlechtesten schneiden alle Firmen in der Kategorie **„Existenzsicherheit“** ab:

-   keine Firma erreicht mehr als **„D“**
-   kaum klare Notfall- oder Abschaltpläne
-   Kontrollproblem ungelöst:  
    Wie bindet man zunehmend autonome Systeme an menschliche Werte?

Die Gutachter sprechen von einer **„fundamentally unprepared“** Branche.

Auch dort, wo Bio- und Cybertests existieren, sind sie:

-   oft nicht standardisiert
-   selten mit verbindlichen Kriterien verknüpft
-   kaum durch externe Stellen geprüft

Das FLI betont ein zentrales Dilemma:

> Transparenz ermöglicht unabhängige Audits –  
> erhöht aber das Risiko illegaler Weiterverwendung.

Der Abstand zwischen Modellfähigkeiten und Schutzarchitektur wächst jedes Quartal.

---

## EU will nachjustieren

Am **2. August 2025** greift die nächste Stufe des europäischen **AI Act**  
([Infos](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)).

Pflichten für General-Purpose-Modelle:

-   Analyse systemischer Risiken
-   Offenlegung zentraler Trainingsdaten
-   dokumentierte Abschaltmechanismen
-   für Hochrisikoanwendungen:
    -   menschliche Aufsicht
    -   Protokollierung
-   Sanktionen bis zu **7 % des globalen Umsatzes**

Ein ergänzender **Code of Practice** soll gefährliche Fähigkeiten gezielt prüfen.  
Google & OpenAI wollen ihn unterschreiben, Meta zögert.

Brüssel bezeichnet das Paket als Blaupause für globale Standards – und als direkte Antwort auf die Lücken, die der FLI-Index offenlegt  
([Golem-Bericht](https://www.golem.de/news/general-purpose-ai-neue-eu-regeln-fordern-transparenz-von-ki-modell-anbietern-2508-198752.html)).

**Knackpunkt:**  
Nationale Behörden brauchen Expertise und Rückhalt, um Berichte zu auditieren – und im Ernstfall Marktverbote auszusprechen.

---

## Fazit: Fortschritt im Schneckentempo – Regulierung wird entscheidend

Der FLI-Index beleuchtet ein Dilemma:

> Die Fähigkeiten generativer Modelle wachsen exponentiell –  
> die Sicherheitsarchitektur nur schleppend.

Selbst die beste Note („C+“) bleibt Mittelmaß.  
Die Mehrheit fällt durch.

Europa setzt auf verbindliche Regulierung.  
Die USA verlassen sich vorerst auf **freiwillige Pledges**.

Beide Ansätze könnten wirken – **wenn**:

-   Aufsichtsbehörden koordiniert handeln
-   Modelle auditierbar werden
-   Notfallpläne skalieren
-   Transparenz zur Pflicht wird

Für Unternehmen ist das Signal klar:

> Transparenz ist kein Goodwill mehr, sondern Ticket zum Markt.

Frühe Audits, klare Abschaltkriterien und glaubwürdige Whistleblowing-Kanäle reduzieren Haftungs- und Reputationsrisiken.

Für die Politik heißt das:

> Mindeststandards dürfen nicht freiwillig bleiben.  
> Nur verbindliche Regeln verhindern, dass auch ein „C+“ reine Kosmetik bleibt.
